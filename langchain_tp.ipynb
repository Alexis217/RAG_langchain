{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación Práctica y Exploración de RAG con Langchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementacion Práctica de RAG\n",
    "\n",
    "En esta seccion se implementa desde cero un sistema RAG usando Langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalacion de liberias\n",
    "# pip install langchain langchain_community langchain-text-splitters langchain-ollama langchain-chroma langchain-google-genai chromadb\n",
    "\n",
    "import os\n",
    "\n",
    "apikey = os.getenv(\"API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga y preparación de un documento de ejemplo\n",
    "\n",
    "Para efectos de prueba, creamos un archivo textual con datos random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documento_ejemplo = \"\"\"\n",
    "En un pueblo pequeño, donde las calles olían a pan recién horneado y las ventanas siempre estaban abiertas, vivía Tomás, un chico que coleccionaba sonidos. No objetos, no estampillas: sonidos.\n",
    "Guardaba en frascos de vidrio el eco de una carcajada, el chasquido de una rama al partirse, el silbido del tren al amanecer. Cada frasco tenía una etiqueta y un color distinto, y cuando los agitaba, el sonido volvía a vibrar, como si nunca se hubiera ido.\n",
    "Una tarde, mientras caminaba por la plaza, escuchó algo que jamás había oído: un silencio cálido, profundo, como un abrazo invisible. Era el silencio que aparece justo antes de que alguien diga algo importante.\n",
    "Tomás corrió a su casa, tomó un frasco vacío y trató de capturarlo. Pero al abrirlo, el silencio se escapó entre sus dedos como si nada pudiera contenerlo.\n",
    "Esa noche entendió que algunos sonidos son demasiado grandes para guardarlos. Algunos —como ese silencio especial— están destinados a llenarlo todo, incluso a uno mismo.\n",
    "Y desde entonces, Tomás ya no coleccionó solo sonidos. También coleccionó momentos, porque descubrió que muchas veces son lo mismo.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"El coleccionista de sonidos.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(documento_ejemplo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline RAG: Carga, Split, Embedding, Indexado y QA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IPF-2025\\Desktop\\proyectos\\Python_para_ciencia_de_datos\\Trag\\RAG_langchain_ollama_tp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "loader = TextLoader(\"El coleccionista de sonidos.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "db = Chroma.from_documents(chunks, embedding_model)\n",
    "apik = apikey\n",
    "\n",
    "try:\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", api_key=apik)\n",
    "except Exception as e:\n",
    "    print(f\"Google GenAI no disponible: {e}. Usa Ollama LLM si lo prefieres.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función de pregunta-respuesta sobre el contenido usando RAG**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomás también coleccionó momentos.\n"
     ]
    }
   ],
   "source": [
    "def rag_query(pregunta, db=db, llm=llm, k=3):\n",
    "    docs = db.similarity_search(pregunta, k=k)\n",
    "    contexto = \"\\n\".join([d.page_content for d in docs])\n",
    "    prompt = f\"\"\"Contexto:\n",
    "{contexto}\n",
    "\n",
    "Pregunta: {pregunta}\n",
    "Respuesta:\"\"\".format(contexto=contexto, pregunta=pregunta)\n",
    "    respuesta = llm.invoke(prompt)\n",
    "    return respuesta.content if hasattr(respuesta, 'content') else respuesta\n",
    "\n",
    "pregunta_ejemplo = \"¿Que otra cosa colecciono tomas?\"\n",
    "print(rag_query(pregunta_ejemplo))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. uso de Modelos de Embedding y LLM\n",
    "\n",
    "Se prueban distintos modelos de embeddings y LLM compatibles para observar como recupera documentos y como genera las respuestas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding model: nomic-embed-text\n",
      "-\n",
      "> Fragmento: En un pueblo pequeÃ±o, donde las calles olÃ­an a pan reciÃ©n horneado y las vent...\n",
      "\n",
      "Embedding model: embeddinggemma\n",
      "-\n",
      "> Fragmento: En un pueblo pequeÃ±o, donde las calles olÃ­an a pan reciÃ©n horneado y las vent...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "embedding_configs = [\n",
    "    (\"nomic-embed-text\", OllamaEmbeddings(model=\"nomic-embed-text\")),                  \n",
    "    (\"embeddinggemma\", OllamaEmbeddings(model=\"embeddinggemma\")),                              \n",
    "]\n",
    "\n",
    "query_text = \"inteligencia artificial en agricultura\"\n",
    "\n",
    "for name, emb in embedding_configs:\n",
    "    try:\n",
    "        db_temp = Chroma.from_documents(chunks, emb, persist_directory=f\"./chroma_{name}\")\n",
    "        docs_found = db_temp.similarity_search(query_text, k=2)\n",
    "        print(f\"\\nEmbedding model: {name}\\n-\")\n",
    "        for d in docs_found:\n",
    "            print(f\"> Fragmento: {d.page_content[:80]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error con embedding {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding: nomic-embed-text | Tiempo: 1.342 seg\n",
      "Fragmento 1: En un pueblo pequeÃ±o, donde las calles olÃ­an a pan reciÃ©n horneado y las vent\n",
      "Fragmento 2: En un pueblo pequeÃ±o, donde las calles olÃ­an a pan reciÃ©n horneado y las vent\n",
      "\n",
      "Embedding: embeddinggemma | Tiempo: 1.835 seg\n",
      "Fragmento 1: En un pueblo pequeÃ±o, donde las calles olÃ­an a pan reciÃ©n horneado y las vent\n",
      "Fragmento 2: En un pueblo pequeÃ±o, donde las calles olÃ­an a pan reciÃ©n horneado y las vent\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Tiempo (seg)</th>\n",
       "      <th>Fragmento 1</th>\n",
       "      <th>Fragmento 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nomic-embed-text</td>\n",
       "      <td>1.342</td>\n",
       "      <td>En un pueblo pequeÃ±o, donde las calles olÃ­an...</td>\n",
       "      <td>En un pueblo pequeÃ±o, donde las calles olÃ­an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>embeddinggemma</td>\n",
       "      <td>1.835</td>\n",
       "      <td>En un pueblo pequeÃ±o, donde las calles olÃ­an...</td>\n",
       "      <td>En un pueblo pequeÃ±o, donde las calles olÃ­an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Modelo  Tiempo (seg)  \\\n",
       "0  nomic-embed-text         1.342   \n",
       "1    embeddinggemma         1.835   \n",
       "\n",
       "                                         Fragmento 1  \\\n",
       "0  En un pueblo pequeÃ±o, donde las calles olÃ­an...   \n",
       "1  En un pueblo pequeÃ±o, donde las calles olÃ­an...   \n",
       "\n",
       "                                         Fragmento 2  \n",
       "0  En un pueblo pequeÃ±o, donde las calles olÃ­an...  \n",
       "1  En un pueblo pequeÃ±o, donde las calles olÃ­an...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "benchmark_results = []\n",
    "query_text = \"inteligencia artificial experta en cuentos\"\n",
    "\n",
    "for name, emb in embedding_configs:\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        db_temp = Chroma.from_documents(chunks, emb, persist_directory=f\"./chroma_{name}\")\n",
    "        docs_found = db_temp.similarity_search(query_text, k=2)\n",
    "        elapsed = time.time() - start_time\n",
    "        fragmentos = [d.page_content[:80] for d in docs_found]\n",
    "        benchmark_results.append({\n",
    "            \"Modelo\": name,\n",
    "            \"Tiempo (seg)\": round(elapsed, 3),\n",
    "            \"Fragmento 1\": fragmentos[0] if len(fragmentos) > 0 else \"\",\n",
    "            \"Fragmento 2\": fragmentos[1] if len(fragmentos) > 1 else \"\"\n",
    "        })\n",
    "        print(f\"\\nEmbedding: {name} | Tiempo: {elapsed:.3f} seg\")\n",
    "        for i, frag in enumerate(fragmentos):\n",
    "            print(f\"Fragmento {i+1}: {frag}\")\n",
    "    except Exception as e:\n",
    "        benchmark_results.append({\"Modelo\": name, \"Tiempo (seg)\": \"Error\", \"Fragmento 1\": str(e), \"Fragmento 2\": \"\"})\n",
    "        print(f\"Error con embedding {name}: {e}\")\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(benchmark_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> google-genai (gemini-2.5-flash)\n",
      "El cuento trata sobre un niño llamado Tomás que colecciona sonidos en frascos. Un día descubre un silencio especial, un silencio \"cálido y profundo\", que intenta capturar sin éxito. A partir de esta experiencia, Tomás comprende que algunos sonidos son demasiado grandes para ser contenidos y que algunos momentos son tan valiosos como los sonidos mismos, por lo que decide coleccionar también momentos. En esencia, el cuento trata sobre el descubrimiento de la importancia de vivir el presente y apreciar las experiencias más allá de intentar poseerlas.\n",
      "\n",
      ">>> qwen2.5:0.5b\n",
      "El cuento trata sobre un niño chico llamado Tomás que es muy interesante en cuanto al arte. El protagonista, un joven de 12 años, busca coleccionar sus propios sonidos para luego guardarlos. Este se llama \"sonido\" y es parte del proceso de coleccionar cosas. Tomás vive en una casa pequeño en un pueblo lejano donde los chinos cocinan pan recién hurgado y las ventanas siempre están abiertas, pero al mismo tiempo disfruta de sus propios sonidos que se hacen eco en el aire.\n",
      "\n",
      "El cuento explica cómo Tomás crea frascos para guardar sus coleccionados y descubre que algunos sonidos son demasiado grandes para ser guardados por él o cualquier otro niño. Como resultado, esta persona decides no comprar más sonidos y, al mismo tiempo, decide no buscar más sonidos en su propio espacio.\n",
      "\n",
      "En el fondo del cuento, se desafía a los lectores encontrar un tema que explique cómo un personaje puede volverse completamente diferente, desde su propia experiencia hasta su estilo de vida. El lector se siente involucrado en una historia que explora los sentidos y la creatividad del autor.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm_models = [\n",
    "    (\"google-genai (gemini-2.5-flash)\", llm),\n",
    "    (\"qwen2.5:0.5b\", ChatOllama(model=\"qwen2.5:0.5b\")),\n",
    "]\n",
    "\n",
    "for nombre, modelo_llm in llm_models:\n",
    "    try:\n",
    "        print(f\"\\n>>> {nombre}\")\n",
    "        respuesta = rag_query(\"¿De que trata el cuento?\", db=db, llm=modelo_llm)\n",
    "        print(respuesta)\n",
    "    except Exception as e:\n",
    "        print(f\"Error usando modelo {nombre}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Optimizacion del Separador de Texto\n",
    "\n",
    "Se prueba variación en los parametros `chunk_size` y `chunk_overlap` del CharacterTextSplitter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size=500, chunk_overlap=100 => chunks: 1\n",
      "> Ejemplo de chunk: En un pueblo pequeÃ±o, donde las calles olÃ­an a pan reciÃ©n...\n",
      "\n",
      "chunk_size=1000, chunk_overlap=200 => chunks: 1\n",
      "> Ejemplo de chunk: En un pueblo pequeÃ±o, donde las calles olÃ­an a pan reciÃ©n...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sizes = [(500, 100), (1000, 200)]\n",
    "\n",
    "for cs, co in sizes:\n",
    "    splitter = CharacterTextSplitter(chunk_size=cs, chunk_overlap=co)\n",
    "    pedazos = splitter.split_documents(docs)\n",
    "    print(f\"chunk_size={cs}, chunk_overlap={co} => chunks: {len(pedazos)}\")\n",
    "    print(f\"> Ejemplo de chunk: {pedazos[0].page_content[:60]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Gestionar Bases de Datos Vectoriales\n",
    "\n",
    "Esta seccion es para la persistencia, colecciones y búsquedas avanzadas con Chroma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base de datos Chroma guardada en ./chroma_db\n"
     ]
    }
   ],
   "source": [
    "persist_path = \"./chroma_db\"\n",
    "db_persist = Chroma.from_documents(chunks, embedding_model, persist_directory=persist_path)\n",
    "\n",
    "print(f\"Base de datos Chroma guardada en {persist_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.078 | Fragmento: En un pueblo pequeÃ±o, donde las calles olÃ­an a pan reciÃ©n horneado ...\n"
     ]
    }
   ],
   "source": [
    "for d in chunks:\n",
    "    d.metadata[\"origen\"] = \"El coleccionista de sonidos.txt\"\n",
    "\n",
    "db_filtrada = Chroma.from_documents(chunks, embedding_model)\n",
    "results = db_filtrada.similarity_search_with_score(\"frascos\", k=5, filter={\"origen\": \"El coleccionista de sonidos.txt\"})\n",
    "for doc, score in results:\n",
    "    print(f\"Score: {score:.3f} | Fragmento: {doc.page_content[:70]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Refinamiento de Prompts\n",
    "\n",
    "Experimentamos con diferentes plantillas y técnicas de prompt engineering para mejorar la calidad, relevancia y trazabilidad de las respuestas generadas por el LLM dentro del sistema RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomás es \"un chico que coleccionaba sonidos. No objetos, no estampillas: sonidos\".\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "base_prompt = \"\"\"Eres un experto en Inteligencia Artificial especializado en análisis de textos.\n",
    "Tu tarea es responder únicamente basándote en el contenido del bloque {context}.\n",
    "Debes citar textualmente las partes del texto que fundamenten tu respuesta cuando sea pertinente.\n",
    "No añadas información externa, no interpretes más allá de lo que el texto permite y no inventes datos ausentes.\n",
    "\n",
    "Texto a analizar:\n",
    "{context}\n",
    "\n",
    "Pregunta sobre el texto:\n",
    "{question}\n",
    "\n",
    "Respuesta (basada estrictamente en el texto):\"\"\"\n",
    "plantilla = PromptTemplate(template=base_prompt, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "def rag_query_template(pregunta, db=db, llm=llm, k=3, prompt_template=plantilla):\n",
    "    docs = db.similarity_search(pregunta, k=k)\n",
    "    contexto = \"\\n\".join([d.page_content for d in docs])\n",
    "    prompt = prompt_template.format(context=contexto, question=pregunta)\n",
    "    respuesta = llm.invoke(prompt)\n",
    "    return respuesta.content if hasattr(respuesta, 'content') else respuesta\n",
    "\n",
    "pregunta_2 = \"¿Quien es tomás?\"\n",
    "print(rag_query_template(pregunta_2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_langchain_ollama_tp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
